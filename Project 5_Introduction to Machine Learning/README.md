In this project, I explored supervised machine learning by implementing logistic regression on a synthetic classification dataset and evaluating it using k-fold and repeated k-fold cross-validation. I systematically tested different numbers of splits and repeats to understand their impact on model accuracy and stability, observing that the default settings often provided the most consistent results. To enhance performance, I applied feature scaling using StandardScaler within a pipeline, which helped standardize input features and improve the consistency of accuracy scores across cross-validation folds.

I also performed hyperparameter tuning on the logistic regression model by optimizing the regularization parameter C with grid search, which further refined model performance while minimizing overfitting. Additionally, I visualized the distribution of accuracy scores with box plots to assess the effect of repeated cross-validation and scaling on model reliability. Overall, this project strengthened my understanding of model evaluation techniques, data preprocessing, and the practical considerations needed to improve supervised learning models in real-world scenarios.
