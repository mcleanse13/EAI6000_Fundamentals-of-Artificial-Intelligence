In this project, I explored Bayesian probabilistic reasoning and implemented Naive Bayes classification methods in Python to solve both numerical and text-based classification problems. I began by learning how supervised learning works, using labeled data to train models and predict outcomes for unseen data. I implemented Gaussian and Multinomial Naive Bayes models to classify synthetic blob data, the Iris plant dataset, and text messages from selected newsgroup categories. Through these exercises, I applied Bayesâ€™ theorem to calculate posterior probabilities and make predictions based on feature distributions, gaining hands-on experience with probabilistic modeling in real-world scenarios.

This project helped me understand the strengths and limitations of Naive Bayes, including its assumptions about feature independence and the effects on model performance. I visualized distributions, calculated accuracy scores, and explored situations where standard accuracy metrics can be misleading, such as with imbalanced datasets. Additionally, I integrated text preprocessing with feature extraction using CountVectorizer and pipelines to build end-to-end classification models. Overall, this capstone deepened my understanding of how symbolic and probabilistic methods can be applied to AI problems, reinforcing both theory and practical skills in Python-based machine learning.
